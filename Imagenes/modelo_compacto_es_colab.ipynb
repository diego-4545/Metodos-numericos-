{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d32b60",
   "metadata": {},
   "source": [
    "# üìö Notebook: Modelo compacto (espa√±ol) entrenado con Wikipedia\n",
    "Este notebook est√° listo para ejecutar en Google Colab. Incluye:\n",
    "\n",
    "- Descarga de un peque√±o subconjunto de Wikipedia en espa√±ol\n",
    "- Preprocesamiento y tokenizaci√≥n\n",
    "- Fine-tuning de `distilgpt2` con correcciones (pad token, resize embeddings)\n",
    "- Par√°metros de generaci√≥n que reducen repeticiones\n",
    "- Interfaz Gradio con sliders para ajustar generaci√≥n en tiempo real\n",
    "\n",
    "‚ö†Ô∏è Nota: ejecutar el entrenamiento puede tardar y consumir recursos. Si usas Colab gratuito, ajusta los par√°metros para reducir el coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf35169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets gradio wikipedia torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import wikipedia\n",
    "import gradio as gr\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad87187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolectar_wikipedia_es(subset_percent='0.05%'):\n",
    "    ds = load_dataset('wikipedia', '20220301.es', split=f\"train[:{subset_percent}]\")\n",
    "    textos = [t for t in ds['text'] if (t and len(t.strip())>200)]\n",
    "    return textos\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    texto = re.sub(r'\\[[^\\]]*\\]', '', texto)\n",
    "    texto = re.sub(r'==.*?==', '', texto)\n",
    "    return texto.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Descargando un subconjunto peque√±o de Wikipedia (es) ‚Äî puede tardar unos minutos...')\n",
    "textos = recolectar_wikipedia_es('0.05%')\n",
    "print(f'Se descargaron {len(textos)} p√°ginas (filtradas).')\n",
    "textos_limpios = [limpiar_texto(t) for t in textos]\n",
    "N = 200\n",
    "textos_entrenamiento = textos_limpios[:N]\n",
    "print(f'Usando {len(textos_entrenamiento)} textos para entrenamiento (puedes cambiar N).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_id = 'distilgpt2'\n",
    "print('Cargando tokenizer y modelo...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_id)\n",
    "modelo = AutoModelForCausalLM.from_pretrained(modelo_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    modelo.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "modelo.config.pad_token_id = modelo.config.get('pad_token_id', tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "print('Tokenizador y modelo listos. Longitud vocab:', len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = Dataset.from_dict({'text': textos_entrenamiento})\n",
    "max_length = 128\n",
    "def tokenizar_batch(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "print('Tokenizando...')\n",
    "dataset_tokenizado = raw_ds.map(tokenizar_batch, batched=True, remove_columns=['text'])\n",
    "print(dataset_tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./modelo_compacto_es',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_tokenizado\n",
    ")\n",
    "print('Listo para entrenar. Ejecuta trainer.train() para iniciar el fine-tuning (puede tardar).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1768f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# trainer.save_model('./modelo_compacto_es')\n",
    "print('Si quieres entrenar, descomenta trainer.train() y trainer.save_model(...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4295482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "def generar_texto(prompt: str, max_length: int=80, temperature: float=1.0, top_p: float=0.92, top_k: int=50, repetition_penalty: float=1.8, no_repeat_ngram_size: int=3):\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        output = modelo.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa269a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=generar_texto,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=2, label='Escribe una frase inicial (prompt)'),\n",
    "        gr.Slider(minimum=20, maximum=200, step=10, value=80, label='max_length'),\n",
    "        gr.Slider(minimum=0.1, maximum=2.0, step=0.1, value=1.0, label='temperature'),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, step=0.01, value=0.92, label='top_p'),\n",
    "        gr.Slider(minimum=1, maximum=200, step=1, value=50, label='top_k'),\n",
    "        gr.Slider(minimum=1.0, maximum=3.0, step=0.1, value=1.8, label='repetition_penalty'),\n",
    "        gr.Slider(minimum=1, maximum=5, step=1, value=3, label='no_repeat_ngram_size')\n",
    "    ],\n",
    "    outputs=gr.Textbox(label='Texto generado'),\n",
    "    title='üß† Modelo compacto (es) - prueba interactiva',\n",
    "    description='Ajusta los sliders para controlar la generaci√≥n y evitar repeticiones.'\n",
    ")\n",
    "\n",
    "iface.launch(enable_flagging=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
